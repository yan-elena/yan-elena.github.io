---
---

@InProceedings{Yan2023b,
  author       = {Elena Yan and
                  Samuele Burattini and
                  Jomi Fred H{\"{u}}bner and
                  Alessandro Ricci},
  editor       = {Rino Falcone and
                  Cristiano Castelfranchi and
                  Alessandro Sapienza and
                  Filippo Cantucci},
  title        = {Towards a Multi-Level Explainability Framework for Engineering and
                  Understanding {BDI} Agent Systems},
  booktitle    = {Proceedings of the 24th Workshop "From Objects to Agents", Roma, Italy,
                  November 6-8, 2023},
  series       = {{CEUR} Workshop Proceedings},
  volume       = {3579},
  pages        = {216--231},
  publisher    = {CEUR-WS.org},
  year         = {2023},
  url          = {https://ceur-ws.org/Vol-3579/paper17.pdf},
  bibtex_show  = {true},
  selected     = {true}
}

@misc{Yan2024,
  author       = {Elena Yan and
                  Luis G. Nardin and
                  Jomi F. H{\"{u}}bner and
                  Olivier Boissier},
  title        = {An Agent-Centric Perspective on Norm Enforcement and Sanctions},
  journal      = {CoRR},
  volume       = {abs/2403.15128},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.15128},
  doi          = {10.48550/ARXIV.2403.15128},
  eprinttype   = {arXiv},
  eprint       = {2403.15128},
  bibtex_show  = {true}
}

@article{Yan2025a,
  author       = {Elena Yan and
                  Samuele Burattini and
                  Jomi Fred H{\"{u}}bner and
                  Alessandro Ricci},
  title        = {A multi-level explainability framework for engineering and understanding
                  {BDI} agents},
  journal      = {Autonomous Agents and Multi-Agent Systems},
  volume       = {39},
  number       = {1},
  pages        = {9},
  year         = {2025},
  url          = {https://doi.org/10.1007/s10458-025-09689-6},
  doi          = {10.1007/S10458-025-09689-6},
  bibtex_show  = {true},
  selected     = {true}
}

@InProceedings{Yan2025b,
  author       = {Yan, Elena and 
                  Nardin, Luis G. and 
                  H{\"u}bner, Jomi F. and 
                  Boissier, Olivier},
  editor       = {Cranefield, Stephen and 
                  Nardin, Luis Gustavo and 
                  Lloyd, Nathan},
  title        = {An Agent-Centric Perspective on Norm Enforcement and Sanctions},
  booktitle    = {Coordination, Organizations, Institutions, Norms, and Ethics for Governance of Multi-Agent Systems XVII},
  year         = {2025},
  publisher    = {Springer Nature Switzerland},
  address      = {Cham},
  pages        = {79--99},
  ISBN         = {978-3-031-82039-7},
  doi          = {10.1007/978-3-031-82039-7_6},
  url          = {https://doi.org/10.1007/978-3-031-82039-7_6},
  bibtex_show  = {true},
  selected     = {true}
}

@thesis{Yan2021,
  title        = {Telemedicina e Wearable Computing a supporto del personale sanitario per la diagnosi dell'ictus: il progetto TeleStroke come caso di studio},
  url          = {https://amslaurea.unibo.it/id/eprint/23876/},
  keywords     = {Wearable Computing,Smartglasses,telemedicina,teleconsulto,TeleStroke,Usabilit{\`a}},
  author       = {Yan, Elena},
  year         = {2021},
  school       = {University of Bologna},
  bibtex_show  = {true},
}

@thesis{Yan2023a,
  title        = {A multi level explainability framework for BDI Multi Agent Systems},
  url          = {https://amslaurea.unibo.it/id/eprint/29644/},
  keywords     = {Agent-oriented software engineering,Multi-Agent Systems,Debugging agent program,Explainability,BDI agents,JaCaMo framework},
  author       = {Yan, Elena},
  abstract     = {As software systems become more complex and the level of abstraction increases, programming and understanding behaviour become more difficult. This is particularly evident in autonomous systems that need to be resilient to change and adapt to possibly unexpected problems, as there are not yet mature tools for managing understanding. A complete understanding of the system is indispensable at every stage of software development, starting with the initial requirements analysis by experts in the field, through to development, implementation, debugging, testing, and product validation. A common and valid approach to increasing understandability in the field of Explainable AI is to provide explanations that can convey the decision making processes and the motivations behind the choices made by the system. Motivated by the different use cases of the explanation and the different classes of target users, it is necessary to deal with different levels of abstraction in the generated explanations since they target specific classes of users with different requirements and goals. This thesis introduces the idea of multi-level explainability as a way to generate different explanations for the same systems at different levels of detail. A low-level explanation related to detailed code could help developers in the debugging and testing phases, while a high-level explanation could support domain experts and designers or contribute to the validation phase to align the system with the requirements. The model taken as a reference for the automatic generation of explanations is the BDI (Belief-Desire-Intention) model, as it would be easier for humans to understand the mentalistic explanation of a system that behaves rationally given its desires and current beliefs. In this work we have prototyped an explainability tool for BDI agents and multi-agent systems that deals with multiple levels of abstraction that can be used for different purposes by different classes of users.},
  year         = {2021},
  school       = {University of Bologna},
  bibtex_show  = {true},
}
